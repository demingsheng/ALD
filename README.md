# Learning Survival Distributions with the Asymmetric Laplace Distribution (ICML 2025)

This repository contains the official implementation of our ICML 2025 paper:

**Learning Survival Distributions with the Asymmetric Laplace Distribution**  
ğŸ“„ [Paper on arXiv](https://arxiv.org/abs/2505.03712)

## ğŸ” Overview
We propose a parametric survival analysis framework based on the Asymmetric Laplace Distribution (ALD), which enables closed-form computation of key distributional summaries such as mean, median, mode, and quantiles. Our method learns individual-level ALD parameters (location, scale, and asymmetry) via maximum likelihood estimation, supporting both observed and censored data. Compared to traditional and neural baselinesâ€”including DeepSurv, DeepHit, and CQRNNâ€”our model achieves superior performance in terms of accuracy, calibration, and discrimination across 21 datasets and 9 evaluation metrics. This approach provides a robust and interpretable alternative for survival modeling in both synthetic and real-world applications.

## ğŸ—‚ï¸ Directory Structure

```
datasets/
â”œâ”€â”€ breast_msk_2018_clinical_data.tsv   
â”œâ”€â”€ gbsg_cancer_train_test.h5            
â”œâ”€â”€ gbsg.csv                             
â”œâ”€â”€ lgggbm_tcga_pub_clinical_data.tsv    
â”œâ”€â”€ metabric_IHC4_clinical_train_test.h5  
â”œâ”€â”€ support_train_test.h5                
â”œâ”€â”€ tmb_immuno_mskcc.tsv                 
â””â”€â”€ whas_train_test.h5                  # real-world data with real censoring datasets

figures/
â”œâ”€â”€ *.png                               # Figures and visualizations generated during analysis

res/
â”œâ”€â”€ *.json                              # Experiment results

./
â”œâ”€â”€ datasets.py                         # Functions for loading and preprocessing datasets
â”œâ”€â”€ hyperparams.py                      # Hyperparameters used in the experiments 
â”œâ”€â”€ models.py                           # Neural network architectures and custom losses in the experiments 
â”œâ”€â”€ script.py                           # Main script for running experiments
â”œâ”€â”€ utils.py                            # Utility functions for common operations

requirements.txt                        # Python dependencies
README.md                               # Project description and usage instructions
```

---

## **Requirements**
Install the required dependencies using the `requirements.txt` file:
```bash
pip install -r requirements.txt
```

---

## **Running Experiments**
Use `script.py` to train and evaluate models:
```bash
python script.py
```

---

## ğŸ“Š Datasets

We evaluate our method on two types of datasets:

### ğŸ”¬ Synthetic Datasets
- **Norm Linear / Non-linear**
- **Exponential**
- **Weibull**
- **LogNorm**
- **Norm-heavy / medium / light**
- **LogNorm-heavy / medium / light / same**

### ğŸ§ª Real-World Datasets
- **METABRIC**: 9 features, 1523 train / 381 test, censoring: 0.42  
- **WHAS**: 6 features, 1310 train / 328 test, censoring: 0.57  
- **SUPPORT**: 14 features, 7098 train / 1775 test, censoring: 0.32  
- **GBSG**: 7 features, 1785 train / 447 test, censoring: 0.42  
- **TMBImmuno**: 3 features, 1328 train / 332 test, censoring: 0.49  
- **BreastMSK**: 5 features, 1467 train / 367 test, censoring: 0.77  
- **LGGGBM**: 5 features, 510 train / 128 test, censoring: 0.60  

Data sources include the [DeepSurv Repository](https://github.com/jaredleekatzman/DeepSurv) and [cBioPortal](https://www.cbioportal.org/).

---

## ğŸ“ Evaluation Metrics

- **Mean Absolute Error (MAE)**
- **Integrated Brier Score (IBS)** 
- **Harrellâ€™s C-Index** 
- **Unoâ€™s C-Index** 
- **Censored D-Calibration (CensDcal)** 
- **Cal[S(t|x)] (Slope / Intercept)**  
- **Cal[f(t|x)] (Slope / Intercept)**

---

## ğŸ§  Models

- **LogNorm MLE** â€“ Parametric baseline using log-normal distribution and maximum likelihood estimation  
- **DSM (LogNorm / Weibull)** â€“ Neural parametric mixture models with log-normal or Weibull components  
- **DeepSurv** â€“ Semi-parametric Cox-based neural model for nonlinear covariate effects  
- **RSF** â€“ Random Survival Forests  
- **GBM** â€“ Gradient Boosted Models adapted for survival  
- **DeepHit** â€“ Non-parametric neural model predicting discrete-time survival distributions  
- **CQRNN** â€“ Censored quantile regression using Asymmetric Laplace loss  
- **ALD (proposed)** â€“ Our method based on Asymmetric Laplace Distribution

---

## ğŸ“ Citation

If you find this work useful, please cite:

```bibtex
@article{sheng2025learning,
  title={Learning Survival Distributions with the Asymmetric Laplace Distribution},
  author={Sheng, Deming and Henao, Ricardo},
  journal={arXiv preprint arXiv:2505.03712},
  year={2025}
}

---

ğŸ“Œ Note: This paper has been accepted to ICML 2025. The official proceedings citation will be updated once available.
